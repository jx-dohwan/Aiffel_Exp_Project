{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jx-dohwan/Aiffel_Exp_Project/blob/main/Document_search/Document_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "taGttxxvYeh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa02203-ac18-4a67-d429-a049ba90691a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "y0ufuwHdyrzP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "ee89e965d21247d781b2b1b400a17f2b",
            "c37ce261bb7445518ccba4b9657c1e06",
            "57f7a9d831e04e3c8a6c68505dc12681",
            "e65f86e07e2349a39106d8fc59c90045",
            "d7b37561b454457e994189ec8fcb4255",
            "dd33e72228b3450a9da6502be9f7ff2e",
            "a78587974cb7469e8bb5007dc794470d",
            "85e84736467c4d65aba01a5245413308",
            "c54daf4d4d354fbfa0a1957ae64238ff",
            "a71847946e874e90a1b3ce0f3ff934f2",
            "04eaf8b75f804f44b02e0a1d87a3f79a",
            "2edb11dc83a84d488c09cebba77ea998",
            "ae6dd560458b4fa7bdb31e894251da09",
            "0030244c91d94bbba43f8ffc52b9b031",
            "6786a681250c4cb0b3a7026fda7ee1fb",
            "86590591cd96488a9e72d17979834ee8",
            "27ad9335dae24515b2e6c1eb986f9e43",
            "e6ab5117ac7d4b6391db34f0ac06f16d",
            "d046790646fb4091a0e3e15f4bd756b9",
            "f980bba6d71541c788039135a0c47980",
            "5fc9c74479334aa6bd11975cd5ee0227",
            "bef6e473c3444f9f9ea9506038cd184a"
          ]
        },
        "id": "OJhYBvggyw-v",
        "outputId": "a94a3e99-40e0-4e82-8bb3-8026b634cb82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset msmarco-passage (/root/.cache/huggingface/datasets/Tevatron___msmarco-passage/default/0.0.1/1874f5d9ae5257b9dbc7d8f89c76f8d4c321be6b660bb5df208e5e64decfa978)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee89e965d21247d781b2b1b400a17f2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset msmarco-passage-corpus (/root/.cache/huggingface/datasets/Tevatron___msmarco-passage-corpus/default/0.0.1/3d8add51914a7d2b589d09e37f4cd9646ebe4a3bad6d59b4e3702cfcc9941f23)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2edb11dc83a84d488c09cebba77ea998"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "qa_data = load_dataset('Tevatron/msmarco-passage')\n",
        "corpus = load_dataset('Tevatron/msmarco-passage-corpus')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KTEhNVGZy9Uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aff681b-a15e-4427-f08d-5ceb5dc9fe38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
              "        num_rows: 400782\n",
              "    })\n",
              "    dev: Dataset({\n",
              "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
              "        num_rows: 6980\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "qa_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5LlM3NvdzF13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e142fea-a73b-49b3-8602-6c9e3bcd7258"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['docid', 'title', 'text'],\n",
              "        num_rows: 8841823\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UlwfXb0kzGc_"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xVyXmOzzzk-B"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EA7ZzMkDzpDg"
      },
      "outputs": [],
      "source": [
        "# model -> dual encoder\n",
        "\n",
        "from transformers import BertModel\n",
        "from torch import nn\n",
        "\n",
        "class Retriever(nn.Module):\n",
        "    def __init__(self, opts):\n",
        "        super(Retriever, self).__init__()\n",
        "        self.q_encoder = BertModel.from_pretrained('bert-base-uncased') # 768\n",
        "        self.ctx_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.proj_q = nn.Linear(opts.bert_hidden_dim, opts.proj_dim) # MIPS 256\n",
        "        self.proj_ctx = nn.Linear(opts.bert_hidden_dim, opts.proj_dim) # 256\n",
        "        self.layer_norm = nn.LayerNorm(opts.proj_dim)\n",
        "        self.dropout = nn.Dropout(opts.dropout)\n",
        "\n",
        "    def forward(self, q_ids, q_attention_mask, q_type_ids, ctx_ids, ctx_attention_mask, ctx_type_ids): # title [SEP] passage\n",
        "        q_vector = self.get_q_emb(q_ids, q_attention_mask, q_type_ids)[:,0,:]\n",
        "        q_vector_proj = self.proj_q(q_vector)\n",
        "        q_vector_proj = self.layer_norm(self.dropout(q_vector_proj))\n",
        "        ctx_vector = self.get_ctx_emb(ctx_ids, ctx_attention_mask, ctx_type_ids)[:,0,:]\n",
        "        ctx_vector_proj = self.proj_ctx(ctx_vector)\n",
        "        ctx_vector_proj = self.layer_norm(ctx_vector_proj)\n",
        "\n",
        "        return q_vector, ctx_vector\n",
        "\n",
        "    def get_q_emb(self, q_ids, q_attention_mask, q_type_ids):\n",
        "        q_vector = self.q_encoder(input_ids=q_ids, attention_mask=q_attention_mask, token_type_ids=q_type_ids)[:, 0, :]\n",
        "        q_vector_proj = self.proj_q(q_vector)\n",
        "        q_vector_proj = self.layer_norm(self.dropout(q_vector_proj))\n",
        "        return q_vector_proj\n",
        "\n",
        "    def get_ctx_emb(self, ctx_ids, ctx_attention_mask, ctx_type_ids):\n",
        "        ctx_vector = self.ctx_encoder(input_ids=ctx_ids, attention_mask=ctx_attention_mask, token_type_ids=ctx_type_ids)[:, 0, :]\n",
        "        ctx_vector_proj = self.proj_ctx(self.dropout(ctx_vector))\n",
        "        ctx_vector_proj = self.layer_norm(ctx_vector_proj)\n",
        "        return ctx_vector_proj"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch model.py"
      ],
      "metadata": {
        "id": "zp9GxYhoBGz8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class RetrieverDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, split='training'):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        question = self.data[index]['question']\n",
        "        if self.split == 'test':\n",
        "            q = self.tokenizer(question, max_length=40, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "            return index, q['input_ids'].squeeze(0), q['attention_mask'].squeeze(0), q['token_type_ids'].squeeze(0)\n",
        "\n",
        "        ctx = self.tokenizer(self.data[index]['ctx'], max_length=200, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "        q = self.tokenizer(question, max_length=40, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "        return q['input_ids'].squeeze(0), q['attention_mask'].squeeze(0), q['token_type_ids'].squeeze(0), ctx['input_ids'].squeeze(0), ctx['attention_mask'].squeeze(0), ctx['token_type_ids'].squeeze(0)\n",
        "\n",
        "    def get_example(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "\n",
        "class PassageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        token = self.tokenizer(self.data[index][2], self.data[index][1], max_length=200, padding=\"max_length\", truncation=True,\n",
        "                               return_tensors='pt')\n",
        "\n",
        "        return int(self.data[index][0])-1, token['input_ids'].squeeze(0), token['attention_mask'].squeeze(0), token['token_type_ids'].squeeze(0)\n",
        "\n",
        "    def get_example(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "class ReaderDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, n_context=5):\n",
        "        self.data = data\n",
        "        self.question_prefix = \"question:\"\n",
        "        self.passage_prefix = \"context:\"\n",
        "        self.title_prefix = \"title:\"\n",
        "        self.n_context = n_context\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def get_target(self, example):\n",
        "        if 'target' in example:\n",
        "            target = example['target']\n",
        "            return target + ' </s>'\n",
        "        elif 'answers' in example:\n",
        "            return random.choice(example['answers']) + ' </s>'\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.data[index]\n",
        "        question = self.question_prefix + \" \" + example['question']\n",
        "        target = self.get_target(example)\n",
        "\n",
        "        if 'ctxs' in example and self.n_context is not None:\n",
        "            f = self.title_prefix + \" {} \" + self.passage_prefix + \" {}\"\n",
        "            contexts = example['ctxs'][:self.n_context]\n",
        "            passages = [f.format(c['title'], c['text']) for c in contexts]\n",
        "        else:\n",
        "            passages = None\n",
        "\n",
        "\n",
        "        return {\n",
        "            'index' : index,\n",
        "            'question' : question,\n",
        "            'target' : target,\n",
        "            'passages' : passages,\n",
        "        }\n",
        "\n",
        "class BertQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    #def __len__(self):\n",
        "\n",
        "    #def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "class ICTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    #def __len__(self):\n",
        "\n",
        "    #def __getitem__(self, idx):\n",
        "\n",
        "def encode_passages(batch_text_passages, tokenizer, max_length):\n",
        "    passage_ids, passage_masks = [], []\n",
        "    for k, text_passages in enumerate(batch_text_passages):\n",
        "        p = tokenizer.batch_encode_plus(\n",
        "            text_passages,\n",
        "            max_length=max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        passage_ids.append(p['input_ids'][None])\n",
        "        passage_masks.append(p['attention_mask'][None])\n",
        "\n",
        "    passage_ids = torch.cat(passage_ids, dim=0)\n",
        "    passage_masks = torch.cat(passage_masks, dim=0)\n",
        "    return passage_ids, passage_masks.bool()\n",
        "\n",
        "class Collator(object):\n",
        "    def __init__(self, text_maxlength, tokenizer, answer_maxlength=20):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_maxlength = text_maxlength\n",
        "        self.answer_maxlength = answer_maxlength\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        assert(batch[0]['target'] != None)\n",
        "        index = torch.tensor([ex['index'] for ex in batch])\n",
        "        target = [ex['target'] for ex in batch]\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            target,\n",
        "            max_length=self.answer_maxlength if self.answer_maxlength > 0 else None,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True if self.answer_maxlength > 0 else False,\n",
        "        )\n",
        "        target_ids = target[\"input_ids\"]\n",
        "        target_mask = target[\"attention_mask\"].bool()\n",
        "        target_ids = target_ids.masked_fill(~target_mask, -100)\n",
        "\n",
        "        def append_question(example):\n",
        "            if example['passages'] is None:\n",
        "                return [example['question']]\n",
        "            return [example['question'] + \" \" + t for t in example['passages']]\n",
        "        text_passages = [append_question(example) for example in batch]\n",
        "        passage_ids, passage_masks = encode_passages(text_passages,\n",
        "                                                     self.tokenizer,\n",
        "                                                     self.text_maxlength)\n",
        "\n",
        "        return (index, target_ids, target_mask, passage_ids, passage_masks)\n",
        "\n",
        "\n",
        "def load_data(data_path=None, global_rank=-1, world_size=-1):\n",
        "    assert data_path\n",
        "    if data_path.endswith('.jsonl'):\n",
        "        data = open(data_path, 'r')\n",
        "    elif data_path.endswith('.json'):\n",
        "        with open(data_path, 'r') as fin:\n",
        "            data = json.load(fin)\n",
        "    examples = []\n",
        "    for k, example in enumerate(data):\n",
        "        if global_rank > -1 and not k%world_size==global_rank:\n",
        "            continue\n",
        "        if data_path is not None and data_path.endswith('.jsonl'):\n",
        "            example = json.loads(example)\n",
        "        if not 'id' in example:\n",
        "            example['id'] = k\n",
        "        for c in example['ctxs']:\n",
        "            if not 'score' in c:\n",
        "                c['score'] = 1.0 / (k + 1)\n",
        "        examples.append(example)\n",
        "    ## egrave: is this needed?\n",
        "    if data_path is not None and data_path.endswith('.jsonl'):\n",
        "        data.close()\n",
        "\n",
        "    return examples\n",
        "    \n",
        "\n",
        "class RetrieverCollator(object):\n",
        "    def __init__(self, tokenizer, passage_maxlength=200, question_maxlength=40):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.passage_maxlength = passage_maxlength\n",
        "        self.question_maxlength = question_maxlength\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        index = torch.tensor([ex['index'] for ex in batch])\n",
        "\n",
        "        question = [ex['question'] for ex in batch]\n",
        "        question = self.tokenizer.batch_encode_plus(\n",
        "            question,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.question_maxlength,\n",
        "            truncation=True\n",
        "        )\n",
        "        question_ids = question['input_ids']\n",
        "        question_mask = question['attention_mask'].bool()\n",
        "\n",
        "        if batch[0]['scores'] is None or batch[0]['passages'] is None:\n",
        "            return index, question_ids, question_mask, None, None, None\n",
        "\n",
        "        scores = [ex['scores'] for ex in batch]\n",
        "        scores = torch.stack(scores, dim=0)\n",
        "\n",
        "        passages = [ex['passages'] for ex in batch]\n",
        "        passage_ids, passage_masks = encode_passages(\n",
        "            passages,\n",
        "            self.tokenizer,\n",
        "            self.passage_maxlength\n",
        "        )\n",
        "\n",
        "        return (index, question_ids, question_mask, passage_ids, passage_masks, scores)\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 data,\n",
        "                 title_prefix='title:',\n",
        "                 passage_prefix='context:'):\n",
        "        self.data = data\n",
        "        self.title_prefix = title_prefix\n",
        "        self.passage_prefix = passage_prefix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.data[index]\n",
        "        text = self.title_prefix + \" \" + example[2] + \" \" + \\\n",
        "            self.passage_prefix + \" \" + example[1]\n",
        "        return example[0], text\n",
        "\n",
        "class TextCollator(object):\n",
        "    def __init__(self, tokenizer, maxlength=200):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxlength = maxlength\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        index = [x[0] for x in batch]\n",
        "        encoded_batch = self.tokenizer.batch_encode_plus(\n",
        "            [x[1] for x in batch],\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.maxlength,\n",
        "            truncation=True\n",
        "        )\n",
        "        text_ids = encoded_batch['input_ids']\n",
        "        text_mask = encoded_batch['attention_mask'].bool()\n",
        "\n",
        "        return index, text_ids, text_mask"
      ],
      "metadata": {
        "id": "0WTvALwz-HKX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!touch dataset.py"
      ],
      "metadata": {
        "id": "QzfKtiPxRKiX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install KorOpenQA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MhJ8dxp_bEy",
        "outputId": "8138157b-ec99-4af3-f87d-577db82787a5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train, distributed training\n",
        "\n",
        "import sys, os\n",
        "#sys.path.append(os.path.abspath('..'))\n",
        "import torch\n",
        "from model import Retriever\n",
        "import torch.nn as nn\n",
        "#import util\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "from dataset import RetrieverDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "import numpy as np\n",
        "import json\n",
        "import KorOpenQA.evaluation\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "parser = ArgumentParser(description=\"Retriever\")\n",
        "parser.add_argument_group()\n",
        "parser.add_argument('--epochs', type=int, default=1000)\n",
        "parser.add_argument('--total_steps', type=int, default=500000)\n",
        "parser.add_argument('--batch_size', type=int, default=128)\n",
        "parser.add_argument('--save_freq', type=int, default=5000)\n",
        "parser.add_argument('--eval_freq', type=int, default=500)\n",
        "parser.add_argument('--gpus', type=int, default=4)\n",
        "parser.add_argument('--train_data_path', type=str, default='data/train.json')\n",
        "parser.add_argument('--dev_data_path', type=str, default='data/dev.json')\n",
        "parser.add_argument('--checkpoint_path', type=str, default='checkpoint')\n",
        "parser.add_argument('--warmup_steps', type=int, default=1000)\n",
        "parser.add_argument('--scheduler_steps', type=int, default=None,\n",
        "                         help='total number of step for the scheduler, if None then scheduler_total_step = total_step')\n",
        "parser.add_argument('--accumulation_steps', type=int, default=1)\n",
        "parser.add_argument('--dropout', type=float, default=0.1, help='dropout rate')\n",
        "parser.add_argument('--lr', type=float, default=1e-5, help='learning rate')\n",
        "parser.add_argument('--clip', type=float, default=1., help='gradient clipping')\n",
        "parser.add_argument('--optim', type=str, default='adam')\n",
        "parser.add_argument('--scheduler', type=str, default='fixed')\n",
        "parser.add_argument('--weight_decay', type=float, default=0.1)\n",
        "parser.add_argument('--bert_hidden_dim', type=int, default=768)\n",
        "parser.add_argument('--proj_dim', type=int, default=256)\n",
        "parser.add_argument('--node_rank', type=int, default=0)\n",
        "parser.add_argument('--num_nodes', type=int, default=1)\n",
        "parser.add_argument('--multi_node', type=int, default=0)\n",
        "parser.add_argument('--num_gpus', type=int, default=-1)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "def evaluate(model, valid_loader, loss_fn, rank):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    tot = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch_data in enumerate(valid_loader):\n",
        "            q_ids, q_attention_mask, q_type_ids, ctx_ids, ctx_attention_mask, ctx_type_ids = batch_data\n",
        "            bsz, psg_len = ctx_ids.size()\n",
        "            ctx_ids = ctx_ids.view(-1, psg_len)\n",
        "            ctx_attention_mask = ctx_attention_mask.view(-1, psg_len)\n",
        "            ctx_type_ids = ctx_type_ids.view(-1, psg_len)\n",
        "            q_vector, ctx_vector = model(q_ids.to(rank), q_attention_mask.to(rank), q_type_ids.to(rank),\n",
        "                                         ctx_ids.to(rank), ctx_attention_mask.to(rank), ctx_type_ids.to(rank))\n",
        "            sim = torch.matmul(q_vector, ctx_vector.T)\n",
        "            labels = torch.arange(0, bsz, 1, dtype=torch.int64, device=rank)\n",
        "            loss += loss_fn(sim, labels).item()\n",
        "            tot += q_ids.size(0)\n",
        "\n",
        "    return loss/tot\n",
        "\n",
        "def train(rank, world_size):\n",
        "    logger = SummaryWriter()\n",
        "    if args.multi_node:\n",
        "        rank = args.node_rank * world_size + rank\n",
        "        dist.init_process_group(backend='nccl', init_method=\"env://\", rank=rank, world_size=args.num_nodes * world_size)\n",
        "    else:\n",
        "        dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)\n",
        "    model = Retriever(args).to(rank)\n",
        "    model = DDP(model, find_unused_parameters=True)\n",
        "    tokenizer = BertTokenizerFast.from_pretrained('./BERT')\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    scheduler =  KorOpenQA.util.WarmupLinearScheduler(optimizer, warmup_steps=args.warmup_steps, scheduler_steps=args.scheduler_steps, min_ratio=0., fixed_lr=True)\n",
        "    train_examples = KorOpenQA.util.load_data(args.train_data_path)\n",
        "    valid_examples = KorOpenQA.util.load_data(args.dev_data_path)\n",
        "    trainset = RetrieverDataset(train_examples, tokenizer)\n",
        "    validset = RetrieverDataset(valid_examples, tokenizer, split='validation')\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(validset)\n",
        "    train_loader = DataLoader(dataset=trainset, batch_size=args.batch_size, num_workers=4, sampler=train_sampler)\n",
        "    valid_loader = DataLoader(dataset=validset, batch_size=args.batch_size, num_workers=4, sampler=valid_sampler)\n",
        "    tot_step = 0\n",
        "    best_loss = np.inf\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            model.train()\n",
        "            tot_step += 1\n",
        "            q_ids, q_attention_mask, q_type_ids, ctx_ids, ctx_attention_mask, ctx_type_ids = batch_data\n",
        "            bsz, psg_len = ctx_ids.size()\n",
        "            ctx_ids =  ctx_ids.view(-1, psg_len)\n",
        "            ctx_attention_mask = ctx_attention_mask.view(-1, psg_len)\n",
        "            ctx_type_ids = ctx_type_ids.view(-1, psg_len)\n",
        "            q_vector, ctx_vector = model(q_ids.to(rank), q_attention_mask.to(rank), q_type_ids.to(rank), ctx_ids.to(rank), ctx_attention_mask.to(rank), ctx_type_ids.to(rank))\n",
        "            sim = torch.matmul(q_vector, ctx_vector.T) # bs * dim, dim * bs -> bs*bs [[q1p1, q1p2, q1p3 .... q1pn,],\n",
        "                                                                                   #   [q2p1, q2p2, q2p3, .... q3pn], ... [qnp1, ...]]\n",
        "            labels = torch.arange(0, bsz, 1, dtype=torch.int64, device=rank)\n",
        "            loss = loss_fn(sim, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            print ('step train loss : ' + str(tot_step) + ' : ' + str(float(loss.item())/q_ids.size(0)))\n",
        "            logger.add_scalar(\"train_loss\", float(loss.item())/q_ids.size(0), tot_step)\n",
        "\n",
        "            if tot_step % args.eval_freq == 0:\n",
        "                model.eval()\n",
        "                val_loss = evaluate(model, valid_loader, loss_fn, rank)\n",
        "                print('valid loss : ' + str(tot_step) + ' : ' + str(float(val_loss)))\n",
        "                logger.add_scalar(\"valid_loss\", float(loss),     tot_step)\n",
        "                if best_loss > val_loss:\n",
        "                    best_loss = val_loss\n",
        "                    if dist.get_rank() == 0:\n",
        "                        torch.save(model.module.state_dict(), args.checkpoint_path+'/'+str(tot_step)+'_retriever.pt')\n",
        "\n",
        "            if tot_step % args.save_freq == 0:\n",
        "                if dist.get_rank() == 0:\n",
        "                    torch.save(model.module.state_dict(), args.checkpoint_path+'/'+str(tot_step)+'_retriever.pt')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(777)\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '16937'\n",
        "    if not os.path.exists(args.checkpoint_path):\n",
        "        os.mkdir(args.checkpoint_path)\n",
        "    if args.num_gpus == -1:\n",
        "        world_size = torch.cuda.device_count()\n",
        "    else:\n",
        "        world_size = args.num_gpus\n",
        "    # gpu - 4, process - 4, \n",
        "    mp.spawn(train, nprocs=world_size, args=(world_size, ))\n"
      ],
      "metadata": {
        "id": "qOKAtnmiRNV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c pytorch faiss-cpu"
      ],
      "metadata": {
        "id": "2gYDaD5QFt2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indexer\n",
        "# max inner product search\n",
        "import faiss\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "class DenseFlatIndexer(object):\n",
        "    def __init__(self, buffer_size: int = 50000):\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "    def init_index(self, vector_sz: int):\n",
        "        self.index = faiss.IndexFlatIP(vector_sz)\n",
        "\n",
        "    def index_data(self, data):\n",
        "        n = len(data)\n",
        "        # indexing in batches is beneficial for many faiss index types\n",
        "        for i in range(0, n, self.buffer_size):\n",
        "            vectors = np.array(data[1]).astype('float32')\n",
        "            self.index.add(vectors)\n",
        "\n",
        "    def search_knn(self, query_vectors: np.array, top_docs: int):\n",
        "        scores, indexes = self.index.search(query_vectors, top_docs)\n",
        "        return (scores, indexes)\n",
        "\n",
        "    def get_index_name(self):\n",
        "        return \"flat_index\""
      ],
      "metadata": {
        "id": "Aob4jX7A-HSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing\n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import os, json\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import math\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import transformers\n",
        "from KorOpenQA.model.retriever import Retriever\n",
        "from dataset import PassageDataset\n",
        "import KorOpenQA.util\n",
        "import time\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "parser = ArgumentParser(description=\"generate_index\")\n",
        "parser.add_argument_group()\n",
        "\n",
        "parser.add_argument('--batch_size', type=int, default=2048)\n",
        "parser.add_argument('--data_path', type=str, default='data/psgs_w100.tsv')\n",
        "parser.add_argument('--output_path', type=str, default='index')\n",
        "parser.add_argument('--dropout', type=float, default=0.0, help='dropout rate')\n",
        "parser.add_argument('--device', type=str, default='cuda:0')\n",
        "parser.add_argument('--bert_hidden_dim', type=int, default=768)\n",
        "parser.add_argument('--proj_dim', type=int, default=256)\n",
        "parser.add_argument('--shard_size', type=int, default=7)\n",
        "parser.add_argument('--multi', type=int, default=1)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "def embed_passages(model, tokenizer, passages, rank=None):\n",
        "    tot = 0\n",
        "    tot_step = 0\n",
        "    allids, allembeddings = [], []\n",
        "    dataset = PassageDataset(passages, tokenizer)\n",
        "    #if args.multi:\n",
        "        #train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
        "        #dataloader = DataLoader(dataset, shuffle=False, num_workers=4, batch_size=args.batch_size, sampler=train_sampler)\n",
        "    #else:\n",
        "    dataloader = DataLoader(dataset, shuffle=False, num_workers=4, batch_size=args.batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i, batch_data in enumerate(dataloader):\n",
        "            tot_step += 1\n",
        "            ids, ctx_ids, ctx_attention_mask, ctx_type_ids = batch_data\n",
        "            tot += ctx_ids.size(0)\n",
        "            if args.multi:\n",
        "                embeddings = model.module.get_ctx_emb(\n",
        "                    ctx_ids=ctx_ids.to(rank),\n",
        "                    ctx_attention_mask=ctx_attention_mask.to(rank),\n",
        "                    ctx_type_ids=ctx_type_ids.to(rank)\n",
        "                )\n",
        "            else:\n",
        "                embeddings = model.get_ctx_emb(\n",
        "                    ctx_ids=ctx_ids.to(args.device),\n",
        "                    ctx_attention_mask=ctx_attention_mask.to(args.device),\n",
        "                    ctx_type_ids=ctx_type_ids.to(args.device)\n",
        "                )\n",
        "            embeddings = embeddings.cpu()\n",
        "\n",
        "            print (str(tot_step) + ' step done...')\n",
        "            print (str(tot) + ' passages embeded...')\n",
        "            allids.extend([id.item() for id in ids])\n",
        "            allembeddings.append(embeddings)\n",
        "\n",
        "    allembeddings = torch.cat(allembeddings, dim=0).numpy()\n",
        "    return allids, allembeddings\n",
        "\n",
        "\n",
        "def dist_main(rank, world_size, passages):\n",
        "    dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)\n",
        "    shard_size = math.ceil(len(passages)/world_size)\n",
        "    start_index = shard_size * rank\n",
        "    end_index = start_index + shard_size\n",
        "    shard_passages = passages[start_index:end_index]\n",
        "    tokenizer = transformers.BertTokenizerFast.from_pretrained('./BERT')\n",
        "    model = Retriever(args).to(rank)\n",
        "    model.load_state_dict(torch.load('retriever.pt'))\n",
        "    model = DDP(model, find_unused_parameters=True)\n",
        "    model.eval()\n",
        "    allids, allembeddings = embed_passages(model, tokenizer, shard_passages, rank=rank)\n",
        "\n",
        "    output_path = Path(args.output_path)\n",
        "    #save_file = output_path.parent / (output_path.name + '_'+str(rank)+'.txt')\n",
        "    save_file = output_path.parent / (output_path.name + '_'+str(rank)+'.pkl')\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    #with open(save_file, mode='w') as f:\n",
        "    #    for id, embedding in zip(allids, allembeddings):\n",
        "    #        f.write(str(id) + '\\t' + str(list(embedding)) + '\\n')\n",
        "    with open(save_file, mode='wb') as f:\n",
        "        pickle.dump((allids, allembeddings), f, protocol=4)\n",
        "    print (\"index saved...\")\n",
        "\n",
        "def main(passages):\n",
        "    tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "    model = Retriever(args)\n",
        "    model.load_state_dict(torch.load('retriever.pt'))\n",
        "    model.to(args.device)\n",
        "    model.eval()\n",
        "    shard_size = math.ceil(len(passages) / args.shard_size)\n",
        "    for i in range(shard_size):\n",
        "        start_index = shard_size * i\n",
        "        end_index = start_index + shard_size\n",
        "        shard_passages = passages[start_index:end_index]\n",
        "        allids, allembeddings = embed_passages(model, tokenizer, shard_passages)\n",
        "\n",
        "        output_path = Path(args.output_path)\n",
        "        save_file = output_path.parent / (output_path.name + '_' + str(i) + '.pkl')\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(save_file, mode='wb') as f:\n",
        "            pickle.dump((allids, allembeddings), f)\n",
        "        print (\"index saved...\")\n",
        "        del allids\n",
        "        del allembeddings\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    torch.manual_seed(777)\n",
        "    start = time.time()\n",
        "    passages = []\n",
        "    with open('passages.json', 'r') as f:\n",
        "        for line in f:\n",
        "            line = json.loads(line)\n",
        "            passages.append((line['id'], line['text'], line['title']))\n",
        "    if args.multi:\n",
        "        world_size = torch.cuda.device_count()\n",
        "        mp.spawn(dist_main, nprocs=world_size, args=(world_size, passages))\n",
        "    else:\n",
        "        main(passages)\n",
        "    print (\"time : \" + str(time.time()-start))"
      ],
      "metadata": {
        "id": "F62WCxK-FcAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieval\n",
        "import sys, os\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "import torch\n",
        "from KorOpenQA.model.retriever import Retriever\n",
        "import torch.nn as nn\n",
        "import KorOpenQA.util\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "from dataset import RetrieverDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "import numpy as np\n",
        "import json\n",
        "import KorOpenQA.index\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pickle\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "parser = ArgumentParser(description=\"Retriever\")\n",
        "parser.add_argument_group()\n",
        "parser.add_argument('--epochs', type=int, default=1000)\n",
        "parser.add_argument('--total_steps', type=int, default=500000)\n",
        "parser.add_argument('--batch_size', type=int, default=128)\n",
        "parser.add_argument('--save_freq', type=int, default=5000)\n",
        "parser.add_argument('--eval_freq', type=int, default=500)\n",
        "parser.add_argument('--gpus', type=int, default=4)\n",
        "parser.add_argument('--test_data_path', type=str, default='data/test.json')\n",
        "parser.add_argument('--checkpoint_path', type=str, default='checkpoint')\n",
        "parser.add_argument('--dropout', type=float, default=0.0, help='dropout rate')\n",
        "parser.add_argument('--scheduler', type=str, default='fixed')\n",
        "parser.add_argument('--bert_hidden_dim', type=int, default=768)\n",
        "parser.add_argument('--proj_dim', type=int, default=256)\n",
        "parser.add_argument('--device', type=str, default='cuda:0')\n",
        "parser.add_argument('--topk', type=int, default=5)\n",
        "parser.add_argument('--shard_num', type=int, default=4)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "def test():\n",
        "    model = Retriever(args).to(args.device)\n",
        "    model.load_state_dict(torch.load('retriever.pt'))\n",
        "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "    test_examples = KorOpenQA.util.load_data(args.test_data_path)\n",
        "    testset = RetrieverDataset(test_examples, tokenizer, split='test')\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=args.batch_size, num_workers=4, shuffle=False)\n",
        "    tot_step = 0\n",
        "    indexer = KorOpenQA.index.DenseFlatIndexer()\n",
        "    indexer.init_index(args.proj_dim)\n",
        "    for i in range(args.shard_num):\n",
        "        with open('index_'+str(i)+'.pkl', 'rb') as f:\n",
        "            index = pickle.load(f)\n",
        "            indexer.index_data(index)\n",
        "        with open('passage_db.pkl', 'rb') as f:\n",
        "            passage_db = pickle.load(f)\n",
        "        with open('data/result.json', 'w') as f:\n",
        "            result = []\n",
        "            for i, batch_data in enumerate(test_loader):\n",
        "                model.train()\n",
        "                tot_step += 1\n",
        "                idx, q_ids, q_attention_mask, q_type_ids = batch_data\n",
        "\n",
        "                ## query\n",
        "                ## tokenizing\n",
        "                ## q encoder\n",
        "                ## faiss -> topk -> search!!\n",
        "                q_vector = model.get_q_emb(q_ids.to(args.device), q_attention_mask.to(args.device), q_type_ids.to(args.device))\n",
        "                q_vector = q_vector.detach().cpu().numpy()\n",
        "                scores, indexes = indexer.search_knn(q_vector, args.topk)\n",
        "                \n",
        "                # MS-marco -> q-p (gold)\n",
        "                # q index -> p \n",
        "                # MRR -> avg(1/rank)\n",
        "                k = 0\n",
        "                print (\"retrieval...\")\n",
        "\n",
        "                # ODQA -> question & answer (passage ??)\n",
        "                for score, index in zip(scores, indexes):\n",
        "                    temp = {}\n",
        "                    temp['question'] = testset.get_example(idx[k])['question']\n",
        "                    temp['answers'] = testset.get_example(idx[k])['answer']\n",
        "                    temp['ctxs'] = []\n",
        "                    for passage_idx in index:\n",
        "                        temp['ctxs'].append({'title': passage_db[passage_idx][0], 'text': passage_db[passage_idx][1]})\n",
        "                    result.append(temp)\n",
        "                    k+=1\n",
        "\n",
        "            json.dump(result, f)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(777)\n",
        "    test()\n"
      ],
      "metadata": {
        "id": "ST8tlGB3FdD_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSpyutrfLwk0SbxoUVWEF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee89e965d21247d781b2b1b400a17f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c37ce261bb7445518ccba4b9657c1e06",
              "IPY_MODEL_57f7a9d831e04e3c8a6c68505dc12681",
              "IPY_MODEL_e65f86e07e2349a39106d8fc59c90045"
            ],
            "layout": "IPY_MODEL_d7b37561b454457e994189ec8fcb4255"
          }
        },
        "c37ce261bb7445518ccba4b9657c1e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd33e72228b3450a9da6502be9f7ff2e",
            "placeholder": "",
            "style": "IPY_MODEL_a78587974cb7469e8bb5007dc794470d",
            "value": "100%"
          }
        },
        "57f7a9d831e04e3c8a6c68505dc12681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85e84736467c4d65aba01a5245413308",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c54daf4d4d354fbfa0a1957ae64238ff",
            "value": 2
          }
        },
        "e65f86e07e2349a39106d8fc59c90045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71847946e874e90a1b3ce0f3ff934f2",
            "placeholder": "",
            "style": "IPY_MODEL_04eaf8b75f804f44b02e0a1d87a3f79a",
            "value": " 2/2 [00:04&lt;00:00,  4.34s/it]"
          }
        },
        "d7b37561b454457e994189ec8fcb4255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd33e72228b3450a9da6502be9f7ff2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78587974cb7469e8bb5007dc794470d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85e84736467c4d65aba01a5245413308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c54daf4d4d354fbfa0a1957ae64238ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a71847946e874e90a1b3ce0f3ff934f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04eaf8b75f804f44b02e0a1d87a3f79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2edb11dc83a84d488c09cebba77ea998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae6dd560458b4fa7bdb31e894251da09",
              "IPY_MODEL_0030244c91d94bbba43f8ffc52b9b031",
              "IPY_MODEL_6786a681250c4cb0b3a7026fda7ee1fb"
            ],
            "layout": "IPY_MODEL_86590591cd96488a9e72d17979834ee8"
          }
        },
        "ae6dd560458b4fa7bdb31e894251da09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27ad9335dae24515b2e6c1eb986f9e43",
            "placeholder": "",
            "style": "IPY_MODEL_e6ab5117ac7d4b6391db34f0ac06f16d",
            "value": "100%"
          }
        },
        "0030244c91d94bbba43f8ffc52b9b031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d046790646fb4091a0e3e15f4bd756b9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f980bba6d71541c788039135a0c47980",
            "value": 1
          }
        },
        "6786a681250c4cb0b3a7026fda7ee1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc9c74479334aa6bd11975cd5ee0227",
            "placeholder": "",
            "style": "IPY_MODEL_bef6e473c3444f9f9ea9506038cd184a",
            "value": " 1/1 [00:23&lt;00:00, 23.97s/it]"
          }
        },
        "86590591cd96488a9e72d17979834ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ad9335dae24515b2e6c1eb986f9e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6ab5117ac7d4b6391db34f0ac06f16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d046790646fb4091a0e3e15f4bd756b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f980bba6d71541c788039135a0c47980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fc9c74479334aa6bd11975cd5ee0227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef6e473c3444f9f9ea9506038cd184a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}